---
title: "Assessing Disclsoure Risk"
author: "Shaina Trevino"
date: "2/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sdcMicro)
```

#Overview

Under the Family Educational Rights and Privacy Act (FERPA), schools can release data to researchers without parent or student consent if the data is released under a [FERPA study exception](https://studentprivacy.ed.gov/sites/default/files/resource_document/file/FERPA%20Exceptions_HANDOUT_portrait.pdf) with a data sharing agreement or if the data has been properly de-identified. Proper data de-identification involves removing or altering all personally identifiable information (PII) from the data to minimize the disclosure risk of individuals. It is important to note that PII includes not only the obvious identifiable information about individuals (e.g., direct identifiers; names, student IDs, social security number, date of birth, address), but also any other information that can be used to identify an individual alone or in combination with other information that can be linked to the data (i.e., indirect identifiers). Thus, simply removing direct identifiers does not properly de-identify the data. Careful attention must be payed to the identification and altering of indirect identifiers to adequately reduce disclosure risk. 

A disclosure occurs when an intruder can reveal the identity of an individual in the data set or learn confidential information about them. Thus, measuring disclosure risk involves calculating risk metrics related to an intruder's ability to identify individuals (i.e., re-identification risk). After assessing the re-identification risk of the original data, statistical disclosure control techniques can be applied to reduce disclosure risk to an appropriate level. There are no specific guidelines or methods to properly de-identify data because the re-identification risk varies depending on the structure, context, and purpose of the data. There may also be other federal, state, or local laws that require different levels of de-identification. 

For our purposes, we will be walking through the process of statistical disclosure control under FERPA with cross-sectional, student-level micro-data (i.e., one student per row). However, it is important to note that there are certain characteristics of micro-data files that can increase the likelihood of a disclosure (we only need to attend to the first): 

+ Existence of rare or detailed records (e.g., unique characteristics or extreme values)

+ Multilevel structure (i.e., if one level is too detailed, disclosure at other levels could be possible)

+ Linking to external sources (e.g., rare combinations of variables, shared linking keys)

+ Longitudinal structure (i.e., adding records over time could increase disclosure risk)

+ Census data (i.e., if an intruder knows who is in the data, disclosure is more likely) 

Specifically, this blog will explain the basic steps of assessing disclosure risk and applying statistical disclosure control techniques with the `{sdcMicro}` package and briefly mention how to re-assess disclosure risk and data utility after transformation when releasing FERPA-compliant student micro-data. Code and data available on [GitHub](LINKREPO)

## Data Description

This data set was simulated to represent the distributions of administrative student variables that a researcher would potentially request access to. Thus, distributions and descriptive statistics represent a student population, however, relationships (e.g., correlations) among variables are not representative, nor realistic, and should not be interpreted. 

When possible, the variables selected for de-identification and release should be limited to those necessary for data analysis in order to minimize disclosure risk and maximize data utility. Our data set includes a sample of 25,000 students and 13 variables including: 

+ `id`: study-specific randomized ID number for each student

+ `grade_level`: student's grade level

+ `sex`: student sex

+ `race`: student race/ethnicity

+ `gpa`: student's unweighted grade point averages 

+ `econ_dis`: student flagged as economically disadvantaged

+ `disability`: student flagged with a disability (i.e., individual education plan)

+ `dis_cat`: student disability type

+ `lang`: student first language

+ `iss_off`: count of behavior incidents that resulted in in-school suspension

+ `iss_days`: total number of days served in in-school suspension

+ `subj_off`: count of behavior incidents that were classified as subjective (i.e., subjective to interpretation) 

+ `days_absent`: total number of days absent

We will use the `import()` and `here()` function to import our data and tidy it with the `{tidyverse}` package (e.g., modifying character variables to factors and defining missing values). It is important to verify that all variables are classified correctly (e.g., numeric or factor) and missing values are defined as `NA`.  


```{r import}
sim_df <- rio::import(here::here("data", "sim_df.csv")) %>% #import data
  mutate_if(is.character, as.factor) %>% #transform characters to factors
  mutate_all(na_if, "") #transform blank cells to missing

#use str() to view variables and types
str(sim_df)
```

# Preprocessing

Before beginning the disclosure risk assessment, some decisions need to be made about the data and types of risks to be protecting against. Direct identifiers should also be removed or obscured from the data in this step as disclosure risk assessment and control procedures are concerned with protecting indirect identifiers. 

## Understand the Data and Context

It can be very helpful to explore the variables and relations among variables in your data set. Looking at the distributions will give a sense of which variables have rare values and are more risky. It will be necessary to fully understand the structure and distributions of variables when making decisions for applying appropriate statistical disclosure techniques. 

It is also crucial to understand the context in which the data is going to be released as this can affect the de-identification process. For example, data shared under a strict data sharing agreement may need less protection and can have a higher acceptable disclosure risk compared to data that is shared without such agreements or will be widely shared. 

Understanding the data and data context makes it possible to develop what are called "motivated intruder scenarios." These intruder scenarios guide the types of protections that need to be applied to the data to appropriately reduce disclosure risk. To develop an intruder scenario, document the ways in which an individual in your data can be identified by an intruder. For example, our data includes many demographic variables, therefore an intruder could identify someone in our data set if they linked unique demographic variables with public registries that include identifiable information (e.g., voter registration, DMV records). Another scenario that is almost always present is the possibility of an inadvertent disclosure which can occur when there are unique combinations of demographic variables resulting in an accidental disclosure. Developing intruder scenarios can sometimes require the support of a subject matter expert to determine which auxiliary data sets are publicly available and contain similar information to the data set to be de-identified. 

## Selecting Key Variables (PII)

Arguably,  one of the most challenging parts of proper data de-identification under FERPA is the classification indirect identifiers, also called key variables in the data de-identification literature or PII in FERPA. FERPA defines indirect identifiers as including "information that, alone or in combination, is linked or linkable to a specific student that would allow a reasonable person in the school community, who does not have personal knowledge of the relevant circumstances, to identify the student with reasonable certainty" ([FERPA](https://www2.ed.gov/policy/gen/guid/fpco/pdf/ferparegs.pdf), p. 6). This vague definition is the reason why classifying indirect identifiers can be so difficult. 

Classifying variables as indirect identifiers or non-identifiers should include consideration of intruder scenarios, conceptual understanding of variables and potential linkages, and exploring variable risk alone and in combination with one another (e.g., rare values, small cell sizes). It can be helpful to assess disclosure risk multiple times with different sets of indirect identifiers or intruder scenarios to understand how different variables affect disclosure risk (e.g., higher re-identification risk, increased suppression) and may be indirect identifiers. For our purposes, we will assume that all variables can be linked with a specific student when combined with other variables in the data set (e.g., only one Asian female student with disability that speaks German). This is a very conservative approach that defines all "real" PII under FERPA correctly, but may classify non-PII variables as indirect identifiers and thus decrease the utility of the transformed data more than necessary. Often, data utility will decrease drastically the more number of indirect identifiers you define, however, more privacy protection is applied.  

# Disclosure Risk Assessment

Once you understand your data, the environment for release, and the variables that are indirect identifiers, you can begin the process of assessing disclosure risk in your data set. To do this we will use the `{sdcMicro}` package and some functions from `{tidyverse}`. 

## Set up the `{sdcMicro}` Object

The first step of disclosure risk analysis in `{sdcMicro}` is to create an object that tells `R` which continuous and categorical variables should be considered indirect identifiers in your data set. This is implemented with the `createSdcObj()` function (as shown below) which results in a `{sdcMicro}` object with many different layers (e.g., original data, transformed data, risk/utility metrics, and many other configuration settings). Mostly everything that we do to assess risk and de-identify data will involve manipulating this object. 

```{r sdc-obj}
#create object
initial_sdcobj <- createSdcObj(dat = sim_df, #input data
                               keyVars = c("grade_level", "sex", "race", "econ_dis", "disability", "dis_cat", "lang"), #categorical indirect identifiers
                               numVars = c("gpa", "iss_off", "iss_days", "subj_off", "days_absent")) #continuous indirect identifiers



```

For our `{sdcMicro}` object, we specified our original data, the categorical indirect identifiers, and the continuous indirect identifiers. There are many other arguments that you can specify if needed (e.g., sampling weights, PRAM values) - just see the help documentation. 

##Categorical Variables

The methods for disclosure risk assessment and control differ for continuous vs. categorical variables (this is why they are defined separately in the object above). For categorical variables, 

Make sure there are similar unique cases/riskiness in simulated data

explain DRA (cont and cat differences)

### K-anon violations

Define k-anon, mention that is our privacy model with 5 as threshold for public use non FERPA exception
violations, uniqueness, etc.

```{r k-anon-vio-sdcobj}
print(initial_sdcobj)


 
```

```{r kten}

kvio <- kAnon_violations(initial_sdcobj, FALSE, 10)
print(kvio)

```

```{r k-anon-vio-info}
#how does this look when rendered? - want to show size of smallest N table here and when comparing raw and transformed
initial_sdcobj
```



### Global Risk

Define

```{r global-risk-OLD}
print(initial_sdcobj, "risk")
```

global risk = (.1110) = "expected proportion of all individuals in the sample that could be re-identified by an intruder." average potential for successful re-identification is 11.10%.  

number of expected re-identifications = n(25000) * global risk (.1110) = 2,774 - motivated intruder could identify. 

no. with more than average risk




### Individual Risk and Sample Frequencies/Uniqueness

- include table that shows relation of k-anon and individual risk %? or explain verbally

f(k) frequency count of each key

```{r samp-freq-risk}
ind_risk <- data.frame(initial_sdcobj@risk$individual) %>% 
  select(-Fk) #get ind risk and freq counts

ind_risk_df <- cbind(sim_df, ind_risk) #combine with data


```

```{r samp-uniq}
#view data
ind_risk_df %>% 
  arrange(desc(risk)) %>% 
  head(10)
#individuals with high risk
```


#### Unique Variable Response Combinations

can aggregate indivudlas to combos of key variables

```{r agg-freq-keys}
#aggregate for keys (using tidyverse)
var_combos <- ind_risk_df %>% 
  select(id, where(is.factor), risk, fk) %>% 
  group_by(grade_level, sex, race, econ_dis, disability, dis_cat, lang) %>%  #prespecified vector
  summarize(risk = mean(risk),
            freq = mean(fk),
            .groups = "keep")

head(var_combos)

```
```{r agg-uniq}
#sort high risk - filter unique
uniq <- var_combos %>% 
  filter(freq == 1)

head(uniq)

```

define k-anon privacy model and 5 threshold for public use non-FERPA exception

```{r agg-risk}

#filter high risk past threshold
risky <- var_combos %>% 
  filter(freq <= 5) %>% 
  arrange(freq)

head(risky)


#aggregate for keys (combos), not rows
#sort by high risk
```




use this: https://sdcpractice.readthedocs.io/en/latest/measure_risk.html#count-of-individuals-with-risks-larger-than-a-certain-threshold




## Continuous Variables

most often looked at after anonymization and compared. 

uniqueness does not apply - distance/neighbor based measures (record linkage, interval measure) and outlier detection

sdcmicro has functions for comparing afterwards, but not looking at before so we will just explore distributions and outliers. 

#### Explore distributions

Look at distributions - plots

-skewness

-gpa is potentially not identifier, may not need top/bottom code

```{r plot-cont}
#make plot function
#plot all cont vars
#show code for hist() function since it is easy

hist(sim_df$gpa)

hist(sim_df$iss_off)

hist(sim_df$iss_days)

hist(sim_df$days_absent)

#partition into one plot with the 4 windows for knitting

```
Look at these distributions in more detail (below)

### Outlier Detection/Explore Tails

no. oberservations and viewing values in tails (freq table)

"identifying the values of a continuous variable that are larger than a predetermined p%-percentile might help identify outliers, and thus units at greater risk of identification. The value of p depends on the skewness of the data."

```{r outliers}
#calculate frequency (or % of sample) of p% percentile for each cont variable

#view largest and smallest values for top% (and bottom if not skewed)
```

skewness and number of observations in tails (of skewed variables above)

```{r outlier-detect}
#gpa
quantile(sim_df$gpa, c(0, .25, .5, .75, 1), na.rm = TRUE) 
table(sim_df$gpa)
```

```{r outlier-detect2}
#days absent
quantile(sim_df$days_absent, c(0, .25, .5, .75, 1), na.rm = TRUE) #high skew so look at high quantiles
quantile(sim_df$days_absent, c(.80, .85, .90, .95, 1), na.rm = TRUE) #look to chose top code value (trade off of utility and cell size)
table(sim_df$days_absent)

```

seems to already been binned/aggregated so that is already protected as we do not know the exact value for each student - could make sense to make categorical or top/bottom code (below 2, above 3.5 - although other categories seem categorical)

discipline data more skewed

suspension data is more skewed so we will only look at values of top tails

```{r outlier-disc}
#in school suspensions
quantile(sim_df$iss_off, c(.80, .85, .90, .95, 1), na.rm = TRUE)
quantile(sim_df$iss_off, c(.95, .96, .97, .98, .99, 1), na.rm = TRUE)
table(sim_df$iss_off)

```

```{r outlier-disc2}
#suspension days
quantile(sim_df$iss_days, c(.80, .85, .90, .95, 1), na.rm = TRUE)
quantile(sim_df$iss_days, c(.95, .96, .97, .98, .99, 1), na.rm = TRUE)
table(sim_df$iss_days)

```


```{r outlier-disc3}
#subjective offenses
quantile(sim_df$subj_off, c(.80, .85, .90, .95, 1), na.rm = TRUE)
quantile(sim_df$subj_off, c(.95, .96, .97, .98, .99, 1), na.rm = TRUE)
table(sim_df$subj_off)
```

options - severe top code, bin into categories and treat as categorical to keep higher values, restrict access/share under FERPA exception


### A Posteriori Measures - or just mention in intro then talk about in reassess (last) section

distnace/neighbor based measures (record linkage, interval measure)

after anonymization - compares raw with transformed data

later post describes how to compute and interpret interval measure