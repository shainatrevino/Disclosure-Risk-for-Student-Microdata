---
title: "Statistical Disclsoure Control"
author: "Shaina Trevino"
date: "2/26/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sdcMicro)
#references outcomes of other post

```

# Statistical Disclosure Control

The challenge is finding a balance between modifying the data to protect student privacy and making sure the data still stay useful for their intended purpose (e.g., data analysis). 

control for risk - reduce risk, then reassess

explain sdc, brief overview of different types of methods (perturbative vs non perturbative with mention of pros/cons). state for our purpose we don't want to perturb (lose truthfulness) so will recode etc..  


GET INFO FROM TUTORIAL RMD (VIEWING SDCOBJ ETC.)

privacy method - kanonymity (choosing k of 5 to be shared with researchers w/o FERPA exception but with DSA that stipulated data destruction - etc. other things that reduce risk)

## Set up sdcMicro Object

creates object based on key variables that will have many different layers (including risk/utility metrics, original data, transformed data, etc.)

```{r sdc-obj}
#create object
initial_sdcobj <- createSdcObj(dat = sim_df, #input data
                               keyVars = c("grade_level", "sex", "race", "econ_dis", "disability", "dis_cat", "lang"), #categorical key variables
                               numVars = c("gpa", "iss_off", "iss_days", "subj_off", "days_absent")) #continuous key variables



```

## Recode continuous variables

cont variables dealt with differently

could change gpa to intervals, but since middle values have high freq - looks already factored - we will just top and bottom code this to make equal N groups. - may not be as useful to analyst if variable of interest. 

use sdcMicro object we created and save over it (or create new object)

```{r cont-recoding}

#change gpa into intervals based on values we have? 
#possible to group and rename continuous variables? - 1-2 (2 or below); 3.6-4 (above 3.5); others remain same but change to factor variable.  
summary(as.factor(sim_df$gpa))

#top/bottom code with sdcMicro code

#top code
tbcode_sdcobj <- topBotCoding(obj = initial_sdcobj, #initial object
                               column = "gpa", #variable to top/bottom code
                               value = 3, #number that will be top coded (>3 will be recoded)
                               replacement = 3334, #number that will replace top coded values - has to be integer- tidy afterward
                               kind = "top") #specify top or bottom coding

#bottom code
tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object you created above when top coding
                               column = "gpa", 
                               value = 2.33, #<2.33 will be recoded
                               replacement = 12, 
                               kind = "bottom") 

#manual check
summary(as.factor(sim_df$gpa))
summary(as.factor(tbcode_sdcobj@manipNumVars$gpa))

```

check continous variable distribution/freq and mention they could check cross tabs with demos 

Discipline variables - highly skewed. 

based on quantiles above, decided to topcode top 3% of students


```{r disc-tbcode}
#top code
tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object created above when top/bottom coding gpa
                               column = "iss_off",
                               value = 2, #3 and up are top-coded
                               replacement = 333,
                               kind = "top")

tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object created above when top/bottom coding gpa
                               column = "iss_days",
                               value = 2, #3 and up are top-coded
                               replacement = 333,
                               kind = "top")

tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object created above when top/bottom coding gpa
                               column = "subj_off",
                               value = 3, #4 and up are top-coded
                               replacement = 444,
                               kind = "top")

```

```{r tbcode_check, include = FALSE}
#double check
tbcode_sdcobj@manipNumVars
```



look at responses within equavalence classes if worried about risk

IF EDITING DAYS ABSENT VAR - LESS RISKY AND SKEWED THAN DISCIPLINE -  ONLY WORRIED ABOUT PROTECTING THOSE WITH HIGH VALUES OF ABSENSES - MAYBE OVER A WEEK - TOPCODE THAT. 

could use ranges, but for quickness we will just top code based on quantiles/frequencies and what makes sense (over 5 days/ 1 school week)

```{r abs-tbcode}

tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object created above when top/bottom coding gpa
                               column = "days_absent",
                               value = 5, #over 5 days are top-coded
                               replacement = 666,
                               kind = "top")

```


## View initial suppression values for categorical variables

traditional methods remove all rows (13% of data) which reduces data utility and disportportioaltely affects minority subgroups

runtime can be long - computationally intense - more so with more categorical variables and response options

only categorical variables in sdcobject - can inlcude importance ranking

using k anon with k of 5. # of k anon violations from above (inline code?)

code that will take a while to run - WARNING - this takes a while, so be aware and maybe try a subset. caching will save version of object into local folder and access that cache when rerunning that chunk. be careful to not cache certain chunks. 

- can skip step. 

```{r local-suppression}
sdcobj_sup <- kAnon(initial_sdcobj, #sdcMicro object we created above
                    importance=c(1, 3, 2, 4, 5, 6, 7), #importance ranking for categorical keys - dafault is order based on # of response categories will be suppressed more
                    k = c(5)) #k-anonymity value (k = 2)

#save RDS to import so this doesnt have to knit everytime? 
```

View results

- look at number of suppressions (and %)

```{r initial-results}
print(sdcobj_sup)
```

could keep this. not useful if people are interested in small language groups or disability categories. we will try to recode those to see if it will reduce suppression

## Recode categorical variables

not dichotomous

```{r freq, include = FALSE}
#view levels and frequencies
summary(fct_infreq(sim_df$race))

summary(fct_infreq(sim_df$dis_cat))
```

recode vars from above suppression, based on research necessity. assume lang doesnt need this fine of detail for analysis, but keeping as much dis_cat as possible is best. 

recode based on frequencies - english, spanish, other

if dis_cat was not helpful for analysis, suggest excluding that variable. If helpful talk with analysts to see what level of aggregation can be applied

ID/DD then suppress whats left. 

```{r recode-cat}
#Recode lang
sdcobj_recode <- groupAndRename(obj = tbcode_sdcobj, #use object with top bottom codes for continuous vars
                                    var = "lang", #variable to recode
                                    before = c("Other","Chinese", "Vietnamese", "Arabic", "Russian", "German", "Swahili"), #vector of variable raw variable levels
                                    after = c("Other")) #name the new group for after recoding

#Recode dis_cat
sdcobj_recode <- groupAndRename(obj = sdcobj_recode, #use sdcobj_recode object you made above
                                    var = "dis_cat",
                                    before = c("Developmental Delay","Intellectual Disability"),
                                    after = c("Developmental Delay/Intellectual Disability")) 

```




## Local suppression for k-anonymity

```{r k-anon-supp}

sdcobj_final <- kAnon(sdcobj_recode, #object we have been recoding
                      importance=c(1, 3, 2, 4, 5, 6, 7), #importance ranking
                      k = c(5)) #k-anonymity value (k = 5)

print(sdcobj_final)

```



## Additional recoding of categorical variables - other options - to achieve acceptable suppression

could keep going, try more recoding for dis_cat or maybe race, make language dichotomous or take out language variable to see if it makes substantial difference in suppression %s

- tried with language dichotomous, didnt change much ( 1 supreesion for grade level; ~1% less suppression for last two vars - not worth it to not have spanish as option if keeping lang var - could remove completely)

take row out for grade_level suppresion 2 if stop now since no other NA values (check)

```{r}

sdcobj_recode1 <- groupAndRename(obj = sdcobj_recode, #use sdcobj_recode object you made above
                                var = "lang",
                                before = c("Spanish", "Other"),
                                after = c("Other")) 
  

```

# Reassess Risk

Very briefly

### Categorical Variables

k-anon vios and global risk reduction

```{r global-risk-final}
#based off cat vars
print(sdcobj_final, "risk")
```

still a little high, might be suitable for release at the SUF level with restrictions

we have achived k-anon for categorical variables with acceptable suppression. gloabl risk reduced, still high (require DSA with destruction and access requirements)

### Continuous Variables

distance based measures, outlier detection, small cell sizes when combined?

```{r numeric-risk-distanceinterval}
sdcobj_final@risk$numeric
```

```{r num-risk}
print(sdcobj_final, "numrisk")
```

Above shows the percentage of observations that are within an interval centred on its recoded value. The upper bound (93.35%) represents a worse case scenario in which a motivated intruder knows each nearest value. This will be high since we only top coded most values to protect top __ % of students who were outliers. 


"they check
if the original value falls within an interval centered on the masked value. In this case, the
intervals are calculated based on the standard deviation of the variable under consideration."


suggests we protected our outliers because changed 7 % when only top coding and not changing other intervals, data utlity decreased drastically. 

1sd of variable

"The result 1 indicates that all (100 percent) the observations are outside the interval of 0.1 times the standard deviation around the original values."

"The results are satisfactory, especially when keeping in mind that there are only 10 distinct values in the dataset (the means of each of the deciles). All outliers have been recoded. Looking at the proportions of the components, we do not detect any outliers (households with an unusual high or low spending pattern in one component)."



outlier detection less useful here becuase loss of variability/range in continuous variables. Can look at freq tables or distributions again to observe changes/assess acceptability

```{r}
table(anon_df$gpa)

table(anon_df$iss_off)

table(anon_df$iss_days)

table(anon_df$subj_off)

```


### Data Utility - same results with raw data

only explain without any code for now? 

orig data didnt have much relations to compare to since simulated for demonstration purposes. 



regressions with raw and transformed data

correlations

## Resources