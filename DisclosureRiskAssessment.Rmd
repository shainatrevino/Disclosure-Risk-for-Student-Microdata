---
title: "Applied DRA and Control with sdcMicro Draft"
author: "Shaina Trevino"
date: "1/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sdcMicro)
```

need to make report style - with very small explained code chunks - view report templates

#Overview

DRA & SDC

purpose, steps (DRA, SDC, reasses), explanation of data

# Preprocessing


## Import Data

Chose variables, clean data (filter year, aggregate schools), random subsample, simulate data, export for use

randomly simulated, similar descriptives/distributions, correlations are not maintained. 

```{r import, include = FALSE}
sim_df <- rio::import(here::here("data", "sim_df.csv")) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate_all(na_if, "")
#need to deal with missing data for characters/factors that are blank now

summary(sim_df)
psych::describe(sim_df)
```

## Understanding Data and Context

explore and understand data

## Selecting Key Variables (PII)

- different scenarios
- run many times with different keys and see which variables are leading to most risk (e.g., most suppression, most unique keys)
- conservative (time-sensitive) all QIDs if data request is small enough (<20 vars, maybe 50)

## Set up sdcMicro Object

creates object based on key variables that will have many different layers (including risk/utility metrics, original data, transformed data, etc.)

```{r sdc-obj}
#vector of categorical variable names - using tidyverse package
catvars <- sim_df %>% 
  select_if(is.factor) %>% 
  colnames()

#or enter manually - use str() to view variables and types - make sure correct 
str(sim_df)

#create object
df_sdcobj <- createSdcObj(dat = sim_df, #input data
                          keyVars = catvars, #categorical key variables - vector we created previously
                          numVars = c("gpa", "iss_off", "iss_days", "subj_off", "days_absent")) #continuous key variables



```

# Disclosure Risk Assessment

NEED TO FIGURE OUT WHICH RISK ASSESSMENT STUFF YOU CANNOT DO AFTER ALREADY MADE OBJECT - MOST LIKLEY EASIER TO EXPLAIN WITH JUST MAKING SDCOBJECT FIRST AND VIEWING RESULTS OF THAT THEN ASSESSING REST OF RAW DATA RISK (WOULD BE LESS LINES OF CODE) - could assess all risk stuff with sdc object, then say some cannot be assessed with that so another way is to (also try with different sets of QIDs)

explain DRA (cont and cat differences) 

assessing risk

Make sure there are similar unique cases/riskiness in simulated data

no sampling weights - so risk might be overestimated

##Categorical Variables

### K-anon violations
how is it different from below (switch section)

```{r k-anon-vio-sdcobj}
print(df_sdcobj)

kvio <- kAnon_violations(df_sdcobj, FALSE, 10)
print(kvio)

#how does this look when rendered? - want to show size of smallest N table here and when comparing raw and transformed
df_sdcobj 
```

```{r k-anon-vio-OLD}
#sdcMicro function
freq_keys <- freqCalc(sim_df, keyVars = catvars) #creates sdcMicro class object #uses one set of key vars - may need to test with many

#vio k-anon violations
freq_keys
```


### Global Risk

f(k) frequency count of each key

global risk = (.13776) = "expected proportion of all individuals in the sample that could be re-identified by an intruder." average potential for successful re-identification is 13.776%.  

number of expected re-identifications = n(25000) * global risk (.13776) = 3,444 - motivated intruder could identify. 

```{r global-risk-OLD}
print(df_sdcobj, "risk")
```


```{r first-risk-OLD}
#need sdc object
#fk <- 

glrisk <- mean(inriskvec)

glrisk
```

### Individual Risk and Sample Frequencies/Uniqueness

- include table that shows relation of k-anon and individual risk %

```{r samp-freq}
ind_freq <- freq(df_sdcobj, type = "fk")

ind_risk <- data.frame(df_sdcobj@risk$individual)

ind_risk_df <- cbind(sim_df, ind_risk)


```

#### Unique Variable Response Combinations

```{r agg-freq-keys}
#aggregate for keys (using tidyverse)
var_combos <- ind_risk_df %>% 
  select(id, is.factor, risk, fk) %>% 
  group_by(grade_level, sex, race, econ_dis, disability, dis_cat, lang) %>%  #prespecified vector
  summarize(risk = mean(risk),
            freq = mean(fk),
            .groups = "keep")

#sort high risk
uniq <- var_combos %>% 
  filter(freq == 1)

risky <- var_combos %>% 
  filter(freq <= 5) %>% 
  arrange(freq)


#aggregate for keys (combos), not rows
#sort by high risk
```


```{r samp-freq-OLD}

#number of unique rows
freq_keys$n1

#number of rows with freq = 2
freq_keys$n2

#extract frequency counts for each row
counts <- freq_keys$fk

#combine dataframe with frequencies counts for each row
freq_counts <- cbind(sim_df, counts)
head(freq_counts[, c(colnames(sim_df), "counts")])


#aggregate to get frequencies for each key/combination

#dataset with only categorical variables
catdf <- sim_df %>% 
  select_if(is.factor)

#aggregate
agg_counts <- aggregate(counts ~ ., catdf, mean)
nrow(agg_counts) #number of possible keys/combinations (without missing values?)

sum(agg_counts$counts == 1) #unique keys/combinations

sum(agg_counts$counts == 2) #keys/combinations with freq = 2 (half of n2 above since that shows rows and this is the combo)

##above is same info in different ways (maybe dont need aggregate unless youw ant to know number of possible keys - can get another way I am sure)

#filter dataframe by counts = 1 and 2 and view freq tables (compare %s with full data to see which responses are leading to most unique cases) - do it with key dataframe (not rows)



```

use this: https://sdcpractice.readthedocs.io/en/latest/measure_risk.html#count-of-individuals-with-risks-larger-than-a-certain-threshold


```{r ind-risk-OLD}


##INDIVIDUAL RISK CALCULATION 
indivf <- indivRisk(freq_keys)
inriskvec <- indivf$rk

freq_risk <- cbind(freq_counts, inriskvec) #- higher is worse. 1 = unique? indiv risk based on counts 15 = .066 risk

```



## Continuous Variables

most often looked at after anonymization and compared. 

uniqueness does not apply - distance/neighbor based measures (record linkage, interval measure) and outlier detection

sdcmicro has functions for comparing afterwards, but not looking at before so we will just explore distributions and outliers. 

#### Explore distributions

Look at distributions - plots

-skewness

-gpa is potentially not identifier, may not need top/bottom code

```{r plot-cont}
#make plot function
#plot all cont vars
#show code for hist() function since it is easy

hist(sim_df$gpa)

hist(sim_df$iss_off)

hist(sim_df$iss_days)

hist(sim_df$days_absent)

```

### Outlier Detection/Explore Skewness

skewness and number of observations in tails (of skewed variables above)

```{r outlier-detect}
#COPIED DIRECTLY
# 90 percentile for income
perc90 <- quantile(file[,'income'], 0.90, na.rm = TRUE)

# Show the ID of observations with values for income larger than the 90 % percentile\
file[(file[, 'income'] >= perc90), 'ID']


##MY VARS
quantile(sim_df$gpa, c(0, .25, .5, .75, 1), na.rm = TRUE) #seems to already been binned/aggregated so that is already protected as we do not know the exact value for each student - could make sense to make categorical or top/bottom code (below 2, above 3.5 - although other categories seem categorical)

quantile(sim_df$iss_off, c(.95, .95, .97, .98, .99, 1), na.rm = TRUE)
summary(as.factor(sim_df$gpa))

huh <- quantile(sim_df$days_absent, c(0, .25, .5, .75, 1), na.rm = TRUE) #high skew so look at high quantiles

sim_df %>% 
  filter(days_absent >= 10) %>% 
  arrange(desc(days_absent)) %>% 
  head(10)

quantile(sim_df$days_absent, c(.80, .85, .90, .95, 1), na.rm = TRUE) #look to chose top code value (trade off of utility and cell size)

summary(as.factor(sim_df$days_absent)) #bad distribution (need more for 0 and 1 - oh boy...)


#suspension data is more skewed so we will only look at values of top tails
quantile(sim_df$iss_off, c(.80, .85, .90, .95, 1), na.rm = TRUE)
quantile(sim_df$iss_off, c(.95, .95, .97, .98, .99, 1), na.rm = TRUE)
summary(as.factor(sim_df$iss_off))

quantile(sim_df$iss_days, c(.80, .85, .90, .95, 1), na.rm = TRUE)
quantile(sim_df$iss_days, c(.95, .95, .97, .98, .99, 1), na.rm = TRUE)
summary(as.factor(sim_df$iss_days))

quantile(sim_df$subj_off, c(.80, .85, .90, .95, 1), na.rm = TRUE)
quantile(sim_df$subj_off, c(.95, .95, .97, .98, .99, 1), na.rm = TRUE)
summary(as.factor(sim_df$subj_off))

#options - severe top code, bin into categories and treat as categorical to keep higher values, restrict access/share under FERPA exception

```





#### Explore skewness

no. oberservations and viewing values in tails (freq table)

"identifying the values of a continuous variable that are larger than a predetermined p%-percentile might help identify outliers, and thus units at greater risk of identification. The value of p depends on the skewness of the data."

```{r - outliers}
#calculate frequency (or % of sample) of p% percentile for each cont variable

#view largest and smallest values for top% (and bottom if not skewed)
```

### A Posteriori Measures - or just mention in intro then talk about in reassess (last) section

distnace/neighbor based measures (record linkage, interval measure)

after anonymization - compares raw with transformed data

# Statistical Disclosure Control

control for risk - reduce risk, then reassess

explain sdc, brief overview of different types of methods (perturbative vs non perturbative with mention of pros/cons). state for our purpose we don't want to perturb (lose truthfulness) so will recode etc..  


GET INFO FROM TUTORIAL RMD (VIEWING SDCOBJ ETC.)

privacy method - kanonymity (choosing k of 5 to be shared with researchers w/o FERPA exception but with DSA that stipulated data destruction - etc. other things that reduce risk)

## Recode continuous variables

cont variables dealt with differently

```{r cont-recoding}

#change gpa into intervals based on values we have? 
#possible to group and rename continuous variables? - 1-2 (2 or below); 3.6-4 (above 3.5); others remain same but change to factor variable.  
summary(as.factor(sim_df$gpa))

#top/bottom code with sdcMicro code

#top/bottom code with tidyverse and remake sdcobject with that data frame

```


## View initial suppression values for categorical variables

runtime can be long - computationally intense - more so with more categorical variables and response options

only categorical variables in sdcobject - can inlcude importance ranking

using k anon with k of 5. # of k anon violations from above (inline code?)

```{r local-suppression}
sdcobj_sup <- kAnon(df_sdcobj, #sdcMicro object we created above
                    importance=c(1, 3, 2, 4, 5, 6, 7), #importance ranking for categorical keys - dafault is order based on # of response categories will be suppressed more
                    k = c(5)) #k-anonymity value (k = 2)

#save RDS to import so this doesnt have to knit everytime? 
```

View results

- look at number of suppressions (and %)

```{r initial-results}
print(sdcobj_sup)
```


## Recode categorical variables

## Local suppression for k-anonymity

## Additional recoding of categorical variables - other options - to achieve acceptable suppression

# Reassess Risk

```{r outlier-rmd}
#huh <- dRiskRMD(df_sdcobj) #all risk values were 0 - think this needs to compare original and transformed data 
```

