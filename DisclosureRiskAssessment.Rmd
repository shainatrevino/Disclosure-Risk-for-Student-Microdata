---
title: "Applied DRA and Control with sdcMicro Draft - Intro to basic data anonymization with student microdata"
author: "Shaina Trevino"
date: "1/19/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sdcMicro)
```

need to make report style - with very small explained code chunks - view report templates

#Overview

DRA & SDC

purpose, steps (DRA, SDC, reasses), explanation of data

# Preprocessing


## Import Data

Chose variables, clean data (filter year, aggregate schools), random subsample, simulate data, export for use

randomly simulated, similar descriptives/distributions, correlations are not maintained. 

TAKE OUT DAYS ABSENT BECAUSE OF DISTRIBUTION ERROR

```{r import, include = FALSE}
sim_df <- rio::import(here::here("data", "sim_df.csv")) %>% 
  mutate_if(is.character, as.factor) %>% 
  mutate_all(na_if, "")
#need to deal with missing data for characters/factors that are blank now

summary(sim_df)
psych::describe(sim_df)
```

## Understanding Data and Context

explore and understand data

## Selecting Key Variables (PII)

- different scenarios
- run many times with different keys and see which variables are leading to most risk (e.g., most suppression, most unique keys)
- conservative (time-sensitive) all QIDs if data request is small enough (<20 vars, maybe 50)

## Set up sdcMicro Object

creates object based on key variables that will have many different layers (including risk/utility metrics, original data, transformed data, etc.)

```{r sdc-obj}
#vector of categorical variable names - using tidyverse package
catvars <- sim_df %>% 
  select_if(is.factor) %>% 
  colnames()

#or enter manually - use str() to view variables and types - make sure correct 
str(sim_df)

#create object
initial_sdcobj <- createSdcObj(dat = sim_df, #input data
                          keyVars = catvars, #categorical key variables - vector we created previously
                          numVars = c("gpa", "iss_off", "iss_days", "subj_off", "days_absent")) #continuous key variables



```

# Disclosure Risk Assessment

NEED TO FIGURE OUT WHICH RISK ASSESSMENT STUFF YOU CANNOT DO AFTER ALREADY MADE OBJECT - MOST LIKLEY EASIER TO EXPLAIN WITH JUST MAKING SDCOBJECT FIRST AND VIEWING RESULTS OF THAT THEN ASSESSING REST OF RAW DATA RISK (WOULD BE LESS LINES OF CODE) - could assess all risk stuff with sdc object, then say some cannot be assessed with that so another way is to (also try with different sets of QIDs)

explain DRA (cont and cat differences) 

assessing risk

Make sure there are similar unique cases/riskiness in simulated data

no sampling weights - so risk might be overestimated

##Categorical Variables

### K-anon violations
how is it different from below (switch section)

```{r k-anon-vio-sdcobj}
print(initial_sdcobj)

kvio <- kAnon_violations(initial_sdcobj, FALSE, 10)
print(kvio)

#how does this look when rendered? - want to show size of smallest N table here and when comparing raw and transformed
initial_sdcobj 
```

```{r k-anon-vio-OLD}
#sdcMicro function
freq_keys <- freqCalc(sim_df, keyVars = catvars) #creates sdcMicro class object #uses one set of key vars - may need to test with many

#vio k-anon violations
freq_keys
```


### Global Risk

f(k) frequency count of each key

global risk = (.13776) = "expected proportion of all individuals in the sample that could be re-identified by an intruder." average potential for successful re-identification is 13.776%.  

number of expected re-identifications = n(25000) * global risk (.13776) = 3,444 - motivated intruder could identify. 

```{r global-risk-OLD}
print(initial_sdcobj, "risk")
```



### Individual Risk and Sample Frequencies/Uniqueness

- include table that shows relation of k-anon and individual risk %

```{r samp-freq}
ind_freq <- freq(initial_sdcobj, type = "fk")

ind_risk <- data.frame(initial_sdcobj@risk$individual)

ind_risk_df <- cbind(sim_df, ind_risk)


```

#### Unique Variable Response Combinations

```{r agg-freq-keys}
#aggregate for keys (using tidyverse)
var_combos <- ind_risk_df %>% 
  select(id, where(is.factor), risk, fk) %>% 
  group_by(grade_level, sex, race, econ_dis, disability, dis_cat, lang) %>%  #prespecified vector
  summarize(risk = mean(risk),
            freq = mean(fk),
            .groups = "keep")

#sort high risk
uniq <- var_combos %>% 
  filter(freq == 1)

risky <- var_combos %>% 
  filter(freq <= 5) %>% 
  arrange(freq)


#aggregate for keys (combos), not rows
#sort by high risk
```


```{r samp-freq-OLD}

#number of unique rows
freq_keys$n1

#number of rows with freq = 2
freq_keys$n2

#extract frequency counts for each row
counts <- freq_keys$fk

#combine dataframe with frequencies counts for each row
freq_counts <- cbind(sim_df, counts)
head(freq_counts[, c(colnames(sim_df), "counts")])


#aggregate to get frequencies for each key/combination

#dataset with only categorical variables
catdf <- sim_df %>% 
  select_if(is.factor)

#aggregate
agg_counts <- aggregate(counts ~ ., catdf, mean)
nrow(agg_counts) #number of possible keys/combinations (without missing values?)

sum(agg_counts$counts == 1) #unique keys/combinations

sum(agg_counts$counts == 2) #keys/combinations with freq = 2 (half of n2 above since that shows rows and this is the combo)

##above is same info in different ways (maybe dont need aggregate unless youw ant to know number of possible keys - can get another way I am sure)

#filter dataframe by counts = 1 and 2 and view freq tables (compare %s with full data to see which responses are leading to most unique cases) - do it with key dataframe (not rows)



```

use this: https://sdcpractice.readthedocs.io/en/latest/measure_risk.html#count-of-individuals-with-risks-larger-than-a-certain-threshold


```{r ind-risk-OLD}


##INDIVIDUAL RISK CALCULATION 
indivf <- indivRisk(freq_keys)
inriskvec <- indivf$rk

freq_risk <- cbind(freq_counts, inriskvec) #- higher is worse. 1 = unique? indiv risk based on counts 15 = .066 risk

```



## Continuous Variables

most often looked at after anonymization and compared. 

uniqueness does not apply - distance/neighbor based measures (record linkage, interval measure) and outlier detection

sdcmicro has functions for comparing afterwards, but not looking at before so we will just explore distributions and outliers. 

#### Explore distributions

Look at distributions - plots

-skewness

-gpa is potentially not identifier, may not need top/bottom code

```{r plot-cont}
#make plot function
#plot all cont vars
#show code for hist() function since it is easy

hist(sim_df$gpa)

hist(sim_df$iss_off)

hist(sim_df$iss_days)

hist(sim_df$days_absent)

```

### Outlier Detection/Explore Skewness

skewness and number of observations in tails (of skewed variables above)

```{r outlier-detect}
##MY VARS
quantile(sim_df$gpa, c(0, .25, .5, .75, 1), na.rm = TRUE) #seems to already been binned/aggregated so that is already protected as we do not know the exact value for each student - could make sense to make categorical or top/bottom code (below 2, above 3.5 - although other categories seem categorical)

quantile(sim_df$iss_off, c(.95, .96, .97, .98, .99, 1), na.rm = TRUE)
summary(as.factor(sim_df$gpa))

huh <- quantile(sim_df$days_absent, c(0, .25, .5, .75, 1), na.rm = TRUE) #high skew so look at high quantiles

sim_df %>% 
  filter(days_absent >= 10) %>% 
  arrange(desc(days_absent)) %>% 
  head(10)

quantile(sim_df$days_absent, c(.80, .85, .90, .95, 1), na.rm = TRUE) #look to chose top code value (trade off of utility and cell size)

summary(as.factor(sim_df$days_absent)) #bad distribution (need more for 0 and 1 - oh boy...) - take out this variable?


#suspension data is more skewed so we will only look at values of top tails
quantile(sim_df$iss_off, c(.80, .85, .90, .95, 1), na.rm = TRUE)
quantile(sim_df$iss_off, c(.95, .96, .97, .98, .99, 1), na.rm = TRUE)
summary(as.factor(sim_df$iss_off))

quantile(sim_df$iss_days, c(.80, .85, .90, .95, 1), na.rm = TRUE)
quantile(sim_df$iss_days, c(.95, .96, .97, .98, .99, 1), na.rm = TRUE)
summary(as.factor(sim_df$iss_days))

quantile(sim_df$subj_off, c(.80, .85, .90, .95, 1), na.rm = TRUE)
quantile(sim_df$subj_off, c(.95, .96, .97, .98, .99, 1), na.rm = TRUE)
summary(as.factor(sim_df$subj_off))

#options - severe top code, bin into categories and treat as categorical to keep higher values, restrict access/share under FERPA exception

```





#### Explore skewness

no. oberservations and viewing values in tails (freq table)

"identifying the values of a continuous variable that are larger than a predetermined p%-percentile might help identify outliers, and thus units at greater risk of identification. The value of p depends on the skewness of the data."

```{r outliers}
#calculate frequency (or % of sample) of p% percentile for each cont variable

#view largest and smallest values for top% (and bottom if not skewed)
```

### A Posteriori Measures - or just mention in intro then talk about in reassess (last) section

distnace/neighbor based measures (record linkage, interval measure)

after anonymization - compares raw with transformed data

# Statistical Disclosure Control

The challenge is finding a balance between modifying the data to protect student privacy and making sure the data still stay useful for their intended purpose (e.g., data analysis). 

control for risk - reduce risk, then reassess

explain sdc, brief overview of different types of methods (perturbative vs non perturbative with mention of pros/cons). state for our purpose we don't want to perturb (lose truthfulness) so will recode etc..  


GET INFO FROM TUTORIAL RMD (VIEWING SDCOBJ ETC.)

privacy method - kanonymity (choosing k of 5 to be shared with researchers w/o FERPA exception but with DSA that stipulated data destruction - etc. other things that reduce risk)

## Recode continuous variables

cont variables dealt with differently

could change gpa to intervals, but since middle values have high freq - looks already factored - we will just top and bottom code this to make equal N groups. - may not be as useful to analyst if variable of interest. 

use sdcMicro object we created and save over it (or create new object)

```{r cont-recoding}

#change gpa into intervals based on values we have? 
#possible to group and rename continuous variables? - 1-2 (2 or below); 3.6-4 (above 3.5); others remain same but change to factor variable.  
summary(as.factor(sim_df$gpa))

#top/bottom code with sdcMicro code

#top code
tbcode_sdcobj <- topBotCoding(obj = initial_sdcobj, #initial object
                               column = "gpa", #variable to top/bottom code
                               value = 3, #number that will be top coded (>3 will be recoded)
                               replacement = 3334, #number that will replace top coded values - has to be integer- tidy afterward
                               kind = "top") #specify top or bottom coding

#bottom code
tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object you created above when top coding
                               column = "gpa", 
                               value = 2.33, #<2.33 will be recoded
                               replacement = 12, 
                               kind = "bottom") 

#manual check
summary(as.factor(sim_df$gpa))
summary(as.factor(tbcode_sdcobj@manipNumVars$gpa))

```

check continous variable distribution/freq and mention they could check cross tabs with demos 

Discipline variables - highly skewed. 

based on quantiles above, decided to topcode top 3% of students


```{r disc-tbcode}
#top code
tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object created above when top/bottom coding gpa
                               column = "iss_off",
                               value = 2, #3 and up are top-coded
                               replacement = 333,
                               kind = "top")

tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object created above when top/bottom coding gpa
                               column = "iss_days",
                               value = 2, #3 and up are top-coded
                               replacement = 333,
                               kind = "top")

tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object created above when top/bottom coding gpa
                               column = "subj_off",
                               value = 3, #4 and up are top-coded
                               replacement = 444,
                               kind = "top")

```

```{r tbcode_check, include = FALSE}
#double check
tbcode_sdcobj@manipNumVars
```



look at responses within equavalence classes if worried about risk

IF EDITING DAYS ABSENT VAR - LESS RISKY AND SKEWED THAN DISCIPLINE -  ONLY WORRIED ABOUT PROTECTING THOSE WITH HIGH VALUES OF ABSENSES - MAYBE OVER A WEEK - TOPCODE THAT. 

could use ranges, but for quickness we will just top code based on quantiles/frequencies and what makes sense (over 5 days/ 1 school week)

```{r abs-tbcode}

tbcode_sdcobj <- topBotCoding(obj = tbcode_sdcobj, #object created above when top/bottom coding gpa
                               column = "days_absent",
                               value = 5, #over 5 days are top-coded
                               replacement = 666,
                               kind = "top")

```


## View initial suppression values for categorical variables

runtime can be long - computationally intense - more so with more categorical variables and response options

only categorical variables in sdcobject - can inlcude importance ranking

using k anon with k of 5. # of k anon violations from above (inline code?)

code that will take a while to run - WARNING - this takes a while, so be aware and maybe try a subset. caching will save version of object into local folder and access that cache when rerunning that chunk. be careful to not cache certain chunks. 

- can skip step. 

```{r local-suppression}
sdcobj_sup <- kAnon(initial_sdcobj, #sdcMicro object we created above
                    importance=c(1, 3, 2, 4, 5, 6, 7), #importance ranking for categorical keys - dafault is order based on # of response categories will be suppressed more
                    k = c(5)) #k-anonymity value (k = 2)

#save RDS to import so this doesnt have to knit everytime? 
```

View results

- look at number of suppressions (and %)

```{r initial-results}
print(sdcobj_sup)
```

could keep this. not useful if people are interested in small language groups or disability categories. we will try to recode those to see if it will reduce suppression

## Recode categorical variables

not dichotomous

```{r freq, include = FALSE}
#view levels and frequencies
summary(fct_infreq(sim_df$race))

summary(fct_infreq(sim_df$dis_cat))
```

recode vars from above suppression, based on research necessity. assume lang doesnt need this fine of detail for analysis, but keeping as much dis_cat as possible is best. 

recode based on frequencies - english, spanish, other

if dis_cat was not helpful for analysis, suggest excluding that variable. If helpful talk with analysts to see what level of aggregation can be applied

ID/DD then suppress whats left. 

```{r recode-cat}
#Recode lang
sdcobj_recode <- groupAndRename(obj = tbcode_sdcobj, #use object with top bottom codes for continuous vars
                                    var = "lang", #variable to recode
                                    before = c("Other","Chinese", "Vietnamese", "Arabic", "Russian", "German", "Swahili"), #vector of variable raw variable levels
                                    after = c("Other")) #name the new group for after recoding

#Recode dis_cat
sdcobj_recode <- groupAndRename(obj = sdcobj_recode, #use sdcobj_recode object you made above
                                    var = "dis_cat",
                                    before = c("Developmental Delay","Intellectual Disability"),
                                    after = c("Developmental Delay/Intellectual Disability")) 

```




## Local suppression for k-anonymity

```{r k-anon-supp}

sdcobj_final <- kAnon(sdcobj_recode, #object we have been recoding
                      importance=c(1, 3, 2, 4, 5, 6, 7), #importance ranking
                      k = c(5)) #k-anonymity value (k = 5)

print(sdcobj_final)

```



## Additional recoding of categorical variables - other options - to achieve acceptable suppression

could keep going, try more recoding for dis_cat or maybe race, make language dichotomous or take out language variable to see if it makes substantial difference in suppression %s

- tried with language dichotomous, didnt change much ( 1 supreesion for grade level; ~1% less suppression for last two vars - not worth it to not have spanish as option if keeping lang var - could remove completely)

take row out for grade_level suppresion 2 if stop now since no other NA values (check)

```{r}

sdcobj_recode1 <- groupAndRename(obj = sdcobj_recode, #use sdcobj_recode object you made above
                                var = "lang",
                                before = c("Spanish", "Other"),
                                after = c("Other")) 
  

```

# Reassess Risk

Very briefly

### Categorical Variables

k-anon vios and global risk reduction

```{r global-risk-final}
#based off cat vars
print(sdcobj_final, "risk")
```

still a little high, might be suitable for release at the SUF level with restrictions

we have achived k-anon for categorical variables with acceptable suppression. gloabl risk reduced, still high (require DSA with destruction and access requirements)

### Continuous Variables

distance based measures, outlier detection, small cell sizes when combined?

```{r numeric-risk-distanceinterval}
sdcobj_final@risk$numeric
```

```{r num-risk}
print(sdcobj_final, "numrisk")
```

Above shows the percentage of observations that are within an interval centred on its recoded value. The upper bound (93.35%) represents a worse case scenario in which a motivated intruder knows each nearest value. This will be high since we only top coded most values to protect top __ % of students who were outliers. 


"they check
if the original value falls within an interval centered on the masked value. In this case, the
intervals are calculated based on the standard deviation of the variable under consideration."


suggests we protected our outliers because changed 7 % when only top coding and not changing other intervals, data utlity decreased drastically. 

1sd of variable

"The result 1 indicates that all (100 percent) the observations are outside the interval of 0.1 times the standard deviation around the original values."

"The results are satisfactory, especially when keeping in mind that there are only 10 distinct values in the dataset (the means of each of the deciles). All outliers have been recoded. Looking at the proportions of the components, we do not detect any outliers (households with an unusual high or low spending pattern in one component)."



outlier detection less useful here becuase loss of variability/range in continuous variables. Can look at freq tables or distributions again to observe changes/assess acceptability

```{r}
table(anon_df$gpa)

table(anon_df$iss_off)

table(anon_df$iss_days)

table(anon_df$subj_off)

```


### Data Utility - same results with raw data

only explain without any code for now? 

orig data didnt have much relations to compare to since simulated for demonstration purposes. 



regressions with raw and transformed data

correlations

```{r utility-check, include = FALSE}
sim_dfn <- sim_df %>% 
  mutate_if(is.factor, as.numeric)

anon_dfn <- anon_df %>% 
  mutate_if(is.factor, as.numeric)

Hmisc::rcorr(as.matrix(sim_dfn))

Hmisc::rcorr(as.matrix(anon_dfn))

#regressions

fit_orig <- lm(gpa ~ ., data = sim_df)
summary(fit_orig)

fit_anon <- lm(gpa ~ ., data = anon_df)
summary(fit_anon)

```

